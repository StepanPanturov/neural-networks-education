{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Сети Колмогорова-Арнольда (KAN)**\n",
        "---\n",
        "## **1. Историческое развитие**\n",
        "\n",
        "Теорему о представлении непрерывных функций нескольких переменных через суперпозиции непрерывных функций одной переменной и сложения, вставшую в основу KAN, сформулировал Андрей Колмогоров в 1957 году. В 1963 году Владимир Арнольд уточнил представление Колмогорова, показав, как именно можно построить функции от одной переменной, тем самым Арнольд придал теореме Колмогорова практическую форму. Именно этот результат стал известен как теорема Колмогорова-Арнольда.\n",
        "\n",
        "Эта теорема, сформулированная в середине XX века, долгое время считалась теоретически значимой, но практически бесполезной из-за сложности реализации и негладкости внутренних функций, значительно усложняющих обучение.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Математические основы**\n",
        "\n",
        "### **2.1 Теорема Колмогорова-Арнольда**\n",
        "\n",
        "Из теоремы Колмогорова-Арнольда следует, что если $f$ — многомерная непрерывная функция в ограниченной области, то $f$ можно записать как конечную композицию непрерывных функций одной переменной и бинарной операции сложения следующим образом: $$f(x_1, \\dots, x_n) = \\sum_{q=1}^{2n+1} \\Phi_q \\left( \\sum_{p=1}^{n} \\phi_{q,p}(x_p) \\right)\n",
        "$$\n",
        "\n",
        "### **2.2 Проблемы теоремы Колмогорова-Арнольда**\n",
        "\n",
        "Теорему до недавнего времени не рассматривали в качестве основы для создания нейронных сетей по двум причинам:\n",
        "\n",
        "1. **Негладкость одномерных функций.**\n",
        "Теорема предполагает разложение многомерной функции на суперпозицию одномерных, но эти одномерные функции могут быть негладкими. Это делает невозможным применение метода обратного распространения ошибки (backpropagation), который требует дифференцируемости функций.\n",
        "\n",
        "2. **Жёсткая фиксация структуры.**\n",
        "В оригинальной теореме архитектура сети строго ограничена: всего два слоя и $2n + 1$ нейронов в скрытом слое (где $n$ — размерность входных данных). Такая структура слишком проста для большинства практических задач и не обладает гибкостью, необходимой для адаптации к сложным данным.\n",
        "\n",
        "### **2.3 Решение проблем теоремы Колмогорова-Арнольда в KAN**\n",
        "\n",
        "1. **B-сплайны.** Вместо произвольных функций KAN используют гладкие B-сплайны для представления активационных функций. Это гарантирует их дифференцируемость и позволяет применять современные методы обучения. Каждая активационная функция имеет вид:\n",
        "\n",
        "$$\n",
        "\\phi(x) = w_b \\cdot b(x) + w_s \\cdot \\mathrm{spline}(x)\n",
        "$$,\n",
        "\n",
        "$b(x)$ - базовая гладкая функция, $\\mathrm{spline}(x)$ - линейная комбинация B-сплайнов:\n",
        "\n",
        "$$\n",
        "\\mathrm{spline}(x) = \\sum_{i} c_i B_i(x)\n",
        "$$\n",
        "\n",
        "2. **Глубокая композиция.** KAN вводят понятие слоя как матрицы функций:\n",
        "\n",
        "$$\n",
        "Φ_l = \\{\\phi_{l,j,i}\\}, \\quad i = 1..n_{in}, \\quad j = 1.n_{out}\n",
        "$$\n",
        "\n",
        "где $\\phi_{l,j,i}$ - обучаемая 1D функция параметризованная сплайном.\n",
        "\n",
        "Выход KAN вычисляется как композиция L таких слоев:\n",
        "\n",
        "$$\n",
        "\\text{KAN}(\\mathbf{x}) = \\left( \\Phi_{L-1} \\circ \\Phi_{L-2} \\circ \\cdots \\circ \\Phi_0 \\right) \\mathbf{x}\n",
        "$$\n",
        "\n",
        "Что также можно нагляднее представить как:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\sum_{i_{L-1}} \\phi_{L-1, i_L, i_{L-1}} \\left(\n",
        "    \\sum_{i_{L-2}} \\cdots \\left(\n",
        "        \\sum_{i_1} \\phi_{1, i_2, i_1} \\left(\n",
        "            \\sum_{i_0} \\phi_{0, i_1, i_0}(x_{i_0})\n",
        "        \\right)\n",
        "    \\right) \\cdots\n",
        "\\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Основная структура**\n",
        "\n",
        "\n",
        "### **3.1 Представление слоев**\n",
        "Архитектура задаётся списком:\n",
        "\n",
        "$$\n",
        "[n_0, n_1, \\dots, n_L]\n",
        "$$\n",
        "\n",
        "где $n_l$ - количество нейронов в слое $l$.\n",
        "\n",
        "### **3.2 Функция активации**\n",
        "Каждая связь между нейронами $(l, i)$ и $(l + 1, j)$ имеет свою функцию активации:\n",
        "\n",
        "$$\n",
        "\\phi_{l,j,i}: ℝ → ℝ\n",
        "$$\n",
        "\n",
        "Параметризованную как:\n",
        "\n",
        "$$\n",
        "\\phi_{l,j,i}(x) = w_b \\cdot b(x) + w_s \\cdot \\mathrm{spline}(x)\n",
        "$$\n",
        "\n",
        "### **3.3 Вычисления в слое**\n",
        "\n",
        "Выход нейрона (в данном случае нейрона $(l +1, j)$) вычисляется как сумма входящих в него функций:\n",
        "\n",
        "$$\n",
        "x_{l+1, j} = \\sum_{i=1}^{n_l}\\phi_{l,j,i}(x_{l, i})\n",
        "$$\n",
        "\n",
        "Если выражать это в матричной форме:\n",
        "\n",
        "$$\n",
        "x_{l+1} = Φ_lx_l\n",
        "$$\n",
        "\n",
        "где - функциональная матрица:\n",
        "\n",
        "$$\n",
        "\\Phi_l =\n",
        "\\begin{bmatrix}\n",
        "\\phi_{l,1,1} & \\cdots & \\phi_{l,1,n_l} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_{l,n_{l+1},1} & \\cdots & \\phi_{l,n_{l+1},n_l}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### **3.4 Полная сеть**\n",
        "\n",
        "Полная сеть уже была описана выше и выглядит следующим образом:\n",
        "\n",
        "$$\n",
        "\\text{KAN}(\\mathbf{x}) = \\left( \\Phi_{L-1} \\circ \\Phi_{L-2} \\circ \\cdots \\circ \\Phi_0 \\right) \\mathbf{x}\n",
        "$$\n",
        "\n",
        "Или в другой форме:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\sum_{i_{L-1}} \\phi_{L-1, i_L, i_{L-1}} \\left(\n",
        "    \\sum_{i_{L-2}} \\cdots \\left(\n",
        "        \\sum_{i_1} \\phi_{1, i_2, i_1} \\left(\n",
        "            \\sum_{i_0} \\phi_{0, i_1, i_0}(x_{i_0})\n",
        "        \\right)\n",
        "    \\right) \\cdots\n",
        "\\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Сравнение KAN с MLP**\n",
        "\n",
        "Многослойные перцептроны долгое время остаются стандартом для аппроксимации нелинейных функций в глубоком обучении. Благодаря универсальной теореме аппроксимации и простоте реализации, MLP легли в основу практически всех современных нейросетевых архитектур.\n",
        "\n",
        "### 4.1 **Функции активации**\n",
        "- **MLP**: Нелинейные функции активации (ReLU, tanh, сигмоида и т.д.) одинаковые для всего слоя, расположены на узлах.\n",
        "- **KAN**: \tОбучаемые нелинейные B-сплайны разные для каждого входа, расположены на ребрах.\n",
        "\n",
        "### 4.2 **Параметризация**\n",
        "- **MLP**:\n",
        "  - Линейные веса: $W \\in \\mathbb{R}^{n_{l+1} \\times n_l}$\n",
        "  - Фиксированные активации: $\\sigma(Wx + b)$\n",
        "- **KAN**:\n",
        "  - Обучаемые функции: $\\phi_{l,j,i}(x) = w_b b(x) + w_s \\text{spline}(x)$\n",
        "  - Нет линейных весов - только параметры сплайнов\n",
        "\n",
        "### 4.3 **Стратегия аппроксимации**\n",
        "\n",
        "* **MLP**: Глобальная аппроксимация — каждый нейрон участвует в формировании всего выходного пространства, обучение распределяет информацию по всей сети.\n",
        "* **KAN**: Локальная аппроксимация — B-сплайны действуют в ограниченных интервалах, что позволяет точно аппроксимировать функции с локальными особенностями.\n",
        "\n",
        "### 4.4 **Метод обучения**\n",
        "\n",
        "* **MLP**: Сквозное обучение (end-to-end) методом обратного распространения ошибки (backpropagation).\n",
        "* **KAN**: Также используется backpropagation, благодаря дифференцируемости сплайнов.\n",
        "\n",
        "### 4.5 **Скорость обучения**\n",
        "\n",
        "* **MLP**: Обычно быстрее обучается благодаря простоте операций и хорошо проработанным оптимизаторам.\n",
        "* **KAN**: Медленнее из-за более сложной архитектуры и численно затратных операций со сплайнами.\n",
        "\n",
        "### 4.6 **Размер и вычислительная эффективность**\n",
        "\n",
        "* **MLP**: Требует больше параметров, но оптимален по числу FLOPs (Floating Point Operations).\n",
        "* **KAN**: Для достижения сопоставимой точности требует меньше параметров, но больше вычислений из-за стоимости сплайнов (больше FLOPs на одно соединение).\n",
        "\n",
        "### 4.7 **Интерпретируемость**\n",
        "\n",
        "* **MLP**: Низкая — веса и активации сложно интерпретировать, особенно в глубоких сетях.\n",
        "* **KAN**: Выше — каждая функция на ребре явно задана (например, через B-сплайн), что позволяет анализировать вклад каждого входа в выход.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Методология обучения KAN**\n",
        "\n",
        "Обучение KAN строится на тех же принципах, что и обучение классических нейросетей, но с рядом особенностей, связанных с обучаемыми функциями активации и нестандартной структурой параметров.\n",
        "\n",
        "### **5.1 Инициализация параметров**\n",
        "\n",
        "Обучение начинается с случайной инициализации параметров сети. В отличие от MLP, где инициализируются веса и смещения, в KAN инициализируются:\n",
        "\n",
        "* коэффициенты B-сплайнов,\n",
        "* весовые коэффициенты линейной составляющей (если используется),\n",
        "* параметры shortcut-ветви (например, линейное преобразование после SiLU).\n",
        "\n",
        "### **5.2 Прямой и обратный проход**\n",
        "\n",
        "Процесс обучения включает два этапа:\n",
        "\n",
        "1. **Прямой проход (forward pass)**: входные данные проходят через сеть, на каждом ребре применяются индивидуальные B-сплайны, результат агрегируется и формирует выход модели.\n",
        "2. **Обратный проход (backward pass)**: вычисляется ошибка (loss) между предсказанием и истинной меткой, после чего с помощью правила цепочки (chain rule) дифференцирования вычисляются градиенты по всем параметрам.\n",
        "\n",
        "Параметры обновляются с помощью стандартных методов оптимизации, таких как градиентный спуск, стохастический градиентный спуск (SGD) или Adam.\n",
        "\n",
        "### **5.3 Проблемы стабильности и регуляризация**\n",
        "\n",
        "Одной из ключевых сложностей при обучении KAN является обеспечение стабильности и сходимости процесса оптимизации. Это связано с тем, что обучаемые функции активации (B-сплайны) имеют сложную зависимость от параметров и могут вызывать переобучение или нестабильные градиенты.\n",
        "\n",
        "Для решения этих проблем применяются:\n",
        "\n",
        "* **Dropout** — для предотвращения переобучения;\n",
        "* **Weight decay** — для ограничения роста параметров;\n",
        "* **Batch normalization** и **Layer normalization** — для стабилизации распределений активаций и ускорения сходимости.\n",
        "\n",
        "Кроме того, выбор функции потерь, инициализация диапазона B-сплайнов и настройка learning rate играют решающую роль в эффективности обучения.\n",
        "\n",
        "### **5.4 Сквозное обучение**\n",
        "\n",
        "KAN поддерживают сквозное (end-to-end) обучение, при котором все параметры сети обучаются одновременно. Это позволяет интегрировать KAN в современные фреймворки глубокого обучения (например, PyTorch), использовать его вместе с другими архитектурными блоками (CNN, Transformer) и обучать модель без ручной настройки отдельных этапов.\n",
        "\n",
        "---\n",
        "\n",
        "Если нужно, я могу добавить схему или график, поясняющий прямой и обратный проход в KAN.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## **Источники**\n",
        "\n",
        "[1] Liu, Z. (2024). Kolmogorov–Arnold Networks (KANs). arXiv:2404.19756\n",
        "\n",
        "[2] Runpeng Yu, Weihao Yu, and Xinchao Wang (2024). KAN or MLP: A Fairer Comparison. arXiv:2407.16674"
      ],
      "metadata": {
        "id": "ATjuiqs43DbR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FxY3Kb6F8vmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGTqmzpf2_bC"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5AtpX5iwgN93"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}