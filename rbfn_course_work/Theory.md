# **Теоретические основы радиально-базисных нейронных сетей**

## **1. Историческое развитие и математические основы**

### **1.1 Происхождение и эволюция**

Радиально-базисные нейронные сети (РБНС) появились в конце 1980-х годов как значимая альтернатива традиционным архитектурам нейронных сетей. Они были разработаны на основе математической теории интерполяции функций с использованием радиально-базисных функций, особенно в контексте аппроксимации разреженных данных. Фундаментальные работы Брумхеда и Лоу (1988), а также Муди и Даркена (1989) заложили основу для РБНС, предложив архитектуры нейронных сетей, которые использовали радиально-базисные функции в качестве функций активации в скрытом слое.

Развитие РБНС было обусловлено несколькими теоретическими преимуществами:
* Прочная математическая основа в теории аппроксимации
* Более быстрое обучение по сравнению с многослойными персептронами (МСП)
* Способность избегать проблем локальных минимумов, характерных для метода обратного распространения ошибки
* Интерпретируемые представления скрытого слоя

### **1.2 Математические основы**

РБНС базируются на математической теории аппроксимации, в частности, в контексте интерполяции функций. Теоретический фундамент опирается на теорему Ковера и свойство универсальной аппроксимации, которое утверждает, что любая непрерывная функция может быть аппроксимирована с произвольной точностью при достаточном количестве базисных функций.

Аппроксимация основана на принципе, что сложная функция может быть построена как линейная комбинация более простых функций (радиально-базисных функций), центрированных в различных точках входного пространства. Этот подход фундаментально отличается от стратегии глобальной аппроксимации, используемой в традиционных многослойных персептронах.

### **1.3 Основная структура**

Стандартная архитектура РБНС состоит из трех слоев:
1. **Входной слой**: Передает входные признаки непосредственно в скрытый слой без трансформации
2. **Скрытый слой**: Содержит нейроны с радиально-базисными функциями активации
3. **Выходной слой**: Выполняет взвешенную сумму выходов скрытых нейронов, обычно с линейной активацией

Математическое представление выхода РБНС для входного вектора x задается:

$$y(\mathbf{x}) = \sum_{i=1}^{N} w_i \cdot \phi(||\mathbf{x} - \mathbf{c}_i||) + b$$

где:

* $N$ - количество скрытых нейронов
* $w_i$ - выходные веса
* $\mathbf{c}_i$ - центры радиально-базисных функций
* $\phi(\cdot)$ - радиально-базисная функция
* $||\mathbf{x} - \mathbf{c}_i||$ - евклидово расстояние между входным вектором и центром
* $b$ - смещение (bias)


### **1.4 Функции активации**

Определяющей характеристикой РБНС является использование радиально-базисных функций в качестве функций активации. Эти функции формируют выходные значения, которые зависят только от расстояния между входным вектором и фиксированной точкой (центром).

#### **1.4.1 Основные радиально-базисные функции**

Наиболее распространенной функцией активации является **функция Гаусса**:

$$\phi(r) = \exp \left( -\frac{r^2}{2\sigma^2} \right)$$

где:
* $r = ||\mathbf{x} - \mathbf{c}||$ - расстояние от центра
* $\sigma$ - параметр ширины, контролирующий разброс функции

Классические радиально-базисные функции также включают:

* **Мультиквадратичная**: $$\phi(r) = \sqrt{r^2 + \sigma^2}$$
* **Обратная мультиквадратичная**: $$\phi(r) = \frac{1}{\sqrt{r^2+\sigma^2}}$$
* **Тонкопластинчатый сплайн**: $$\phi(r) = r^2 \log(r)$$

#### **1.4.2 Расширенный набор радиально-базисных функций**

В современных исследованиях и практических применениях используется более широкий спектр радиально-базисных функций, каждая из которых обладает уникальными свойствами:

**1. Линейная функция:**
$$\phi(r) = r$$
- Простейшая RBF функция
- Неограниченно возрастающая с увеличением расстояния
- Применяется в задачах, где требуется линейная интерполяция

**2. Гауссова функция:**
$$\phi(r) = \exp\left(-\frac{r^2}{2\sigma^2}\right)$$
- Наиболее популярная RBF функция
- Локализованная (быстро убывает с расстоянием)
- Гладкая и дифференцируемая

**3. Тонкопластинчатый сплайн (Thin-plate Spline):**
$$\phi(r) = r^2 \ln r$$
- Часто используется в задачах интерполяции
- Обеспечивает гладкие переходы между точками
- Популярна в компьютерной графике и обработке изображений

**4. Логистическая функция:**
$$\phi(r) = \frac{1}{1 + \exp\left(\frac{r^2}{\sigma^2}\right)}$$
- Ограниченная функция (значения от 0 до 1)
- Монотонно убывающая
- Подходит для задач классификации

**5. Hardy Multiquadric:**
$$\phi(r) = \frac{1}{(r^2 + \sigma^2)^\alpha}$$
- Обобщение обратной мультиквадратичной функции
- Параметр $\alpha$ контролирует скорость убывания
- Гибкая настройка под конкретные задачи

**6. Мультиквадратичная функция:**
$$\phi(r) = (r^2 + \sigma^2)^\beta$$
- Неограниченно возрастающая функция
- Параметр $\beta$ определяет степень роста
- Эффективна для аппроксимации глобальных трендов

**7. DSP Kernel:**
$$\phi(r) = \frac{1}{1 + \frac{r^2}{\sigma^2}}$$
- Применяется в цифровой обработке сигналов
- Монотонно убывающая функция
- Хорошие свойства сглаживания

**8. Предложенная квадратичная функция:**
$$\phi(r) = \max\left(0, 1 - \frac{r^2}{\sigma^2}\right)$$
- Компактная поддержка (равна нулю вне определенного радиуса)
- Вычислительно эффективная
- Подходит для локальной аппроксимации

#### **1.4.3 Выбор функции активации**

Выбор конкретной радиально-базисной функции зависит от:

- **Характера данных**: гладкие функции для непрерывных данных, компактные для локальных особенностей
- **Требований к вычислительной эффективности**: простые функции для быстрых вычислений
- **Необходимости глобальной или локальной аппроксимации**: глобальные функции для общих трендов, локальные для детализации
- **Свойств сходимости**: некоторые функции обеспечивают лучшую сходимость алгоритмов обучения

Каждая функция имеет свои преимущества и области применения, что делает РБНС гибким инструментом для решения широкого спектра задач машинного обучения.

Недавние исследования Чжао и др. (2019) были посвящены полностью обучаемым базисным функциям активации, позволяющим сетям адаптировать свою нелинейность в процессе обучения для улучшения производительности ([DeepLABNet](https://www.alphaxiv.org/abs/1911.09257)).

## **2. Сети РБФ в сравнении с другими архитектурами нейронных сетей**

### **2.1 Сравнение с многослойными персептронами (МСП)**

РБНС фундаментально отличаются от МСП по нескольким ключевым аспектам:

| Характеристика | РБНС | МСП |
|----------------|------|-----|
| Функции активации | Радиальные (основанные на расстоянии) | Сигмоидальные/ReLU (основанные на скалярном произведении) |
| Стратегия аппроксимации | Локальная (каждый нейрон реагирует на область) | Глобальная (распределенное представление) |
| Метод обучения | Часто двухэтапный (без учителя + с учителем) | Обычно сквозное обучение с учителем |
| Скрытые слои | Обычно один скрытый слой | Множественные скрытые слои |
| Скорость обучения | Как правило, быстрее | В целом медленнее, склонность к локальным минимумам |
| Интерпретируемость | Выше (центры являются прототипами) | Ниже (распределенные представления) |

Ключевое концептуальное различие заключается в способе разделения входного пространства. МСП строят сложные границы решений через иерархические преобразования признаков, в то время как РБНС используют набор локализованных рецептивных полей, создающих области активации вокруг прототипных точек.


### **2.2 Сравнение с сетями Кохонена**

Сети Кохонена (самоорганизующиеся карты или SOM) имеют некоторые сходства с фазой выбора центров РБНС, но существенно отличаются по назначению и структуре:

| Характеристика | РБНС | Сети Кохонена (SOM) |
|----------------|------|---------------------|
| Основное назначение | Аппроксимация функций/классификация | Снижение размерности/кластеризация |
| Выходной слой | Линейная комбинация активаций | Обычно отсутствует (визуализация) |
| Обучение | С учителем (после размещения центров) | Без учителя |
| Функция окрестности | Отсутствует | Критически важна для сохранения топологии |
| Результат | Отображение "вход-выход" | Топологическая карта входного пространства |

Хотя обе сети используют прототипные векторы (центры в РБНС и весовые векторы в SOM), сети Кохонена фокусируются на сохранении топологических свойств входного пространства, в то время как РБНС используют прототипы для построения аппроксимации функции.

### **2.3 Сравнение с современными глубокими сетями**

С развитием глубокого обучения важно понимать, как РБНС соотносятся с современными архитектурами:

| Характеристика | РБНС | Современные глубокие сети |
|----------------|------|---------------------------|
| Глубина | Неглубокие (обычно 3 слоя) | Глубокие (много слоев) |
| Эффективность параметров | Ниже для высокоразмерных данных | Выше благодаря иерархическим признакам |
| Требования к данным | Могут работать с ограниченными данными | Обычно требуют большие наборы данных |
| Сложность обучения | Проще, часто частично аналитическое | Сложное, сквозное на основе градиентов |
| Интерпретируемость | Выше | Ниже |
| Масштабируемость | Проблематично для очень больших наборов данных | Лучше подходят для крупномасштабных задач |

Недавняя работа Менга и др. (2021) попыталась преодолеть этот разрыв путем интеграции компонентов РБФ с сверточными нейронными сетями для улучшения интерпретируемости при сохранении производительности ([RBF-CNN](https://www.alphaxiv.org/abs/2208.11401)).

## **3. Методологии обучения**

Обучение РБНС обычно включает два основных этапа:

### **3.1 Выбор центров**

Существуют три основных подхода к определению центров нейронов РБФ:

1. **Случайный выбор**: Центры случайно выбираются из обучающих данных
2. **Выбор на основе кластеризации**: Центры определяются с помощью алгоритмов кластеризации (например, K-средних) для идентификации прототипических точек во входном пространстве
3. **Выбор с учителем**: Центры изучаются посредством оптимизации на основе градиентного спуска

### **3.2 Определение ширины**

Параметр ширины $\sigma$ значительно влияет на способность сети к обобщению:

1. **Фиксированная ширина**: Все нейроны используют одну и ту же предопределенную ширину
2. **P-ближайший сосед**: Ширина определяется на основе среднего расстояния до p-ближайших центров
3. **Адаптивная ширина**: Каждый нейрон имеет собственный параметр ширины, оптимизируемый во время обучения

### **3.3 Оптимизация выходных весов**
После фиксации центров и ширин выходные веса могут быть оптимизированы с использованием:

1. **Решение в замкнутой форме**: Использование псевдообратной матрицы (оптимально в смысле наименьших квадратов)

  $$W = \Phi^{\dagger}TW = \Phi^+T$$

  где $\Phi$ — матрица выходов скрытого слоя, а $T$ — целевая матрица.

2. **Градиентный спуск**: Для онлайн-обучения или когда решение в замкнутой форме вычислительно затратно
3. **Регуляризованный метод наименьших квадратов**: Для улучшения обобщения

### **3.4 Сквозное обучение**

Недавние подходы перешли к сквозному обучению всех параметров одновременно. Согласно исследованию Чжао и др. (2019), их архитектура DeepLABNet позволяет обучать РБНС на основе градиентного спуска с полностью обучаемыми базисными функциями, обеспечивая интеграцию с современными фреймворками глубокого обучения ([DeepLABNet](https://www.alphaxiv.org/abs/1911.09257)).

## **4. Практические применения**

РБНС нашли применение в многочисленных областях благодаря их эффективному обучению и сильным аппроксимационным возможностям:

### **4.1 Аппроксимация функций и регрессия**

РБНС превосходно справляются с аппроксимацией сложных нелинейных функций, что делает их подходящими для:

* Идентификации систем в теории управления
* Прогнозирования временных рядов в финансах и экономике
* Суррогатного моделирования в инженерной оптимизации

### **4.2 Распознавание образов и классификация**

Их локализованная природа делает РБНС эффективными для задач классификации:

* Распознавание рукописных символов
* Распознавание лиц и биометрия
* Обнаружение аномалий в различных областях

### **4.3 Анализ временных рядов**

Недавняя работа Лю и др. (2024) демонстрирует эффективность РБНС в заполнении пропусков во временных рядах, где они моделируют локальные временные зависимости и надежно обрабатывают паттерны отсутствующих данных [Заполнение пропусков во временных рядах с помощью РБНС](https://www.alphaxiv.org/abs/2407.17040).

### **4.4 Обработка сигналов**

РБНС используются в различных приложениях обработки сигналов:

* Шумоподавление
* Эквализация каналов в коммуникациях
* Интерполяция и реконструкция сигналов

### **4.5 Системы управления**

Быстрое обучение и надежная производительность РБНС делают их ценными в приложениях управления:

* Адаптивное управление
* Планирование траекторий для роботов
* Управление процессами в промышленных условиях

# **5. Сильные стороны и ограничения**

## **5.1 Сильные стороны**

1. **Сильный теоретический фундамент**: Основан на теории аппроксимации с гарантированными свойствами
2. **Быстрое обучение**: Часто быстрее методов, основанных на обратном распространении ошибки, особенно для задач среднего размера
3. **Локальная аппроксимация**: Эффективен при захвате локальных паттернов и обработке нестационарных данных
4. **Интерпретируемость**: Центры могут быть интерпретированы как прототипические примеры из обучающих данных
5. **Избегает локальных минимумов**: Двухэтапный подход к обучению снижает риск субоптимальных решений
6. **Хорошо работает с ограниченными данными**: Может достичь хорошей обобщающей способности с меньшими наборами данных

## **5.2 Ограничения**

1. **Проклятие размерности**: С увеличением размерности входных данных число необходимых RBF-единиц растет экспоненциально
2. **Требования к памяти**: Хранение всех центров и вычисление расстояний может быть ресурсоемким
3. **Проблема выбора центров**: Выбор подходящих центров критически важен, но сложен
4. **Меньшая эффективность параметров**: По сравнению с глубокими сетями для высокоразмерных задач
5. **Интеграция с современными фреймворками**: Традиционные RBFNN не естественно вписываются в конвейеры глубокого обучения

## **5.3 Сравнение с глубокими нейронными сетями**

| Аспект | RBFNN | Глубокие нейронные сети |
|--------|-------|-------------------------|
| Требования к обучающим данным | Могут хорошо работать с меньшим объемом данных | Обычно требуют большие наборы данных |
| Время обучения | В целом быстрее | Обычно дольше из-за большого числа параметров |
| Интерпретируемость | Более интерпретируемы (центры как прототипы) | Часто рассматриваются как "черные ящики" |
| Масштабируемость для сложных задач | Ограничена взрывным ростом числа центров | Лучше подходят благодаря иерархическому обучению |
| Трансферное обучение | Ограниченные возможности | Сильные возможности трансферного обучения |
| Обработка высокоразмерных данных | Сложно без снижения размерности | Более эффективны благодаря иерархическому извлечению признаков |

# **6. Недавние достижения и будущие направления**

Недавние исследования сосредоточены на решении традиционных ограничений RBFNN и их интеграции с современными подходами глубокого обучения:

## **6.1 Интеграция с глубоким обучением**

Исследователи изучают способы включения RBF-слоев в глубокие архитектуры:

* RBF-компоненты в CNN для улучшения интерпретируемости ([Meng et al. 2021](https://alphaxiv.org/abs/2208.11401))
* Полносвязные обучаемые RBF-сети с изучаемыми функциями активации ([Zhao et al. 2019](https://alphaxiv.org/abs/1911.09257))

## **6.2 Гибридные модели**

Комбинирование RBFNN с другими типами нейронных сетей:

* RBF-CNN гибриды для классификации изображений
* RBF-LSTM комбинации для моделирования временных последовательностей
* RBF-компоненты поверх извлекателей признаков

## **6.3 Квантовые RBFNN**

Принципы квантовых вычислений применяются к RBFNN:

* Квантово-вдохновленные RBFNN для классификации данных ([Tang et al. 2019](https://alphaxiv.org/abs/1910.08798))
* Потенциальные ускорения для высокоразмерных задач

## **6.4 Продвинутые приложения**

Продолжают появляться новые приложения:

* Восстановление пропущенных значений временных рядов с помощью многомерных RBFNN ([Liu et al. 2024](https://alphaxiv.org/abs/2407.17040))
* Реализации для периферийных вычислений в IoT-приложениях
* Специализированные приложения для финансового моделирования

# **Заключение**

Радиальные базисные функциональные нейронные сети представляют теоретически обоснованный подход к проектированию нейронных сетей с уникальными свойствами, которые продолжают делать их актуальными в ландшафте машинного обучения. Хотя традиционные RBFNN сталкивались с проблемами, связанными с масштабируемостью и интеграцией с современными фреймворками, недавние достижения в обучаемых функциях активации, гибридных архитектурах и новых приложениях возродили исследовательский интерес.

# **Источники**
1. Broomhead, D. S., & Lowe, D. (1988). Multivariable functional interpolation and adaptive networks. Complex Systems, 2, 321-355.

2. Moody, J., & Darken, C. J. (1989). Fast learning in networks of locally-tuned processing units. Neural Computation, 1(2), 281-294.

3. Meng, Y., et al. (2021). Radial Basis Function Networks for Convolutional Neural Networks to Learn Similarity Distance Metric and Improve Interpretability. arXiv:2208.11401

4. Zhao, M., et al. (2019). DeepLABNet: End-to-end Learning of Deep Radial Basis Networks with Fully Learnable Basis Functions. arXiv:1911.09257

5. Tang, M., et al. (2019). Data classification by quantum radial basis function networks. arXiv:1910.08798

6. Liu, X., et al. (2024). Time Series Imputation with Multivariate Radial Basis Function Neural Network. arXiv:2407.17040

7. Orr, M. J. L. (1996). Introduction to radial basis function networks. Technical Report, Centre for Cognitive Science, University of Edinburgh.

8. Haykin, S. (2009). Neural networks and learning machines (3rd ed.). Pearson.